---
title: "COMMON STATISTICAL TESTS"
output: html_notebook
---
# **1. The simplicity underlying common tests**
Most of the common statistical models (t-test, correlation, ANOVA; chi-square, etc.) are special cases of linear models or a very close approximation. This beautiful simplicity means that there is less to learn. In particular, it all comes down to *y=a⋅x+b* which most students know from high school. Unfortunately, stats intro courses are usually taught as if each test is an independent tool, needlessly making life more complicated for students and teachers alike.

This needless complexity multiplies when students try to rote learn the parametric assumptions underlying each test separately rather than deducing them from the linear model.

For this reason, I think that teaching linear models first and foremost and then name-dropping the special cases along the way makes for an excellent teaching strategy, emphasizing understanding over rote learning. Since linear models are the same across frequentist, Bayesian, and permutation-based inferences, I’d argue that it’s better to start with modeling than p-values, type-1 errors, Bayes factors, or other inferences.

Concerning the teaching of “non-parametric” tests in intro-courses, I think that we can justify lying-to-children and teach “non-parametric”" tests as if they are merely ranked versions of the corresponding parametric tests. It is much better for students to think “ranks!” than to believe that you can magically throw away assumptions. Indeed, the Bayesian equivalents of “non-parametric”" tests implemented in JASP literally just do (latent) ranking and that’s it. For the frequentist “non-parametric”" tests considered here, this approach is highly accurate for N > 15.

--------------------------------------------------------------------------------
--------------------------------------------------------------------------------

# **2. Setting and toy data**
```{r}
# Load packages for data handling and plotting
# install.packages("patchwork")
library(ggplot2)
library(tidyverse)
library(patchwork)
library(broom)

# Reproducible "random" results
set.seed(40)

# Generate normal data with known parameters
rnorm_fixed = function(N, mu = 0, sd = 1)
  scale(rnorm(N)) * sd + mu

# Plot style.
theme_axis = function(P,
                      jitter = FALSE,
                      xlim = c(-0.5, 2),
                      ylim = c(-0.5, 2),
                      legend.position = NULL) {
  P = P + theme_bw(15) +
    geom_segment(
      x = -1000, xend = 1000,
      y = 0, yend = 0,
      lty = 2, color = 'dark gray', lwd = 0.5
    ) +
    geom_segment(
      x = 0, xend = 0,
      y = -1000, yend = 1000,
      lty = 2, color = 'dark gray', lwd = 0.5
    ) +
    coord_cartesian(xlim = xlim, ylim = ylim) +
    theme(
      axis.title = element_blank(),
      axis.text = element_blank(),
      axis.ticks = element_blank(),
      panel.border = element_blank(),
      panel.grid = element_blank(),
      legend.position = legend.position
    )
  
  # Return jittered or non-jittered plot?
  if (jitter) {
    P + geom_jitter(width = 0.1, size = 2)
  }
  else {
    P + geom_point(size = 2)
  }
}
```

```{r}
# Wide format (sort of)
#y = rnorm_fixed(50, mu=0.3, sd=2)  # Almost zero mean.
y = c(rnorm(15), exp(rnorm(15)), runif(20, min = -3, max = 0))  # Almost zero mean, not normal
x = rnorm_fixed(50, mu = 0, sd = 1)  # Used in correlation where this is on x-axis
y2 = rnorm_fixed(50, mu = 0.5, sd = 1.5)  # Used in two means

# Long format data with indicator
value = c(y, y2)
group = rep(c('y1', 'y2'), each = 50)
```

--------------------------------------------------------------------------------
--------------------------------------------------------------------------------
--------------------------------------------------------------------------------

## Pearson and Spearman Correlation
### Theory: As linear models
Model: the recipe for *y* is a slope (*β*~1~) times *x* plus an intercept (*β*~0~, aka a straight line).

*y* = *β*~0~ + *β*~1~x  
*H*~0~: *β*~1~ = 0

… which is a math-y way of writing the good old y = ax + b (here ordered as y=b+ax). In R we are lazy and write y ~ 1 + x which R reads like y = 1 * number + x * othernumber and the task of t-tests, lm, etc., is simply to find the numbers that best predict y.

Either way you write it, it’s an intercept (*β*~0~) and a slope (*β*~1~) yielding a straight line:
```{r} 
set.seed(40)
# Fixed correlation
D_correlation = data.frame(MASS::mvrnorm(30, mu = c(0.9, 0.9), Sigma = matrix(c(1, 0.8, 1, 0.8), ncol = 2), empirical = TRUE))  # Correlated data
# NB: the data frame created above contains 30 observations for two variables: X1 and X2

# Add labels (for next plot)
D_correlation$label_num = sprintf('(%.1f,%.1f)', D_correlation$X1, D_correlation$X2) # creates a new variable that takes on two values for each observation. These values are: 1) X1 rounded to 1 decimal place and 2) X2 rounded to 1 dp.
D_correlation$label_rank = sprintf('(%i,%i)', rank(D_correlation$X1), rank(D_correlation$X2)) # creates a new variable that takes on two values for each observation. These values are 1) the ranking of X1 values from lowest to highest and 2) the ranking of X2 from lowest to highest.
# NB: the sprintf function formats the entries as follows:
#     %.1f: rounds the specified objects [i.e., X1 and X2 values in the current example] to 1 decimal place
#     %i: rounds the specified objects [i.e., the respective ranking of X1 and X2 values] to the nearest integer

# Plot it
fit = lm(I(X2 * 0.5 + 0.4) ~ I(X1 * 0.5 + 0.2), D_correlation) # The I function is simply an "as is" instruction, i.e., the operation inside the parentheses should be performed as specified. It is useful when dealing with complex terms in statistical modeling. For example, y and x are defined respectively as "X2*0.5+0.4" and X2*0.5+0.2 in the current example. The I function ensures that such complex definitions are treated as representation for X, or Y... as the case maybe. 
intercept_pearson = coefficients(fit)[1] # value in the first column of the coefficients output, i.e., the intercept column

P_pearson = ggplot(D_correlation, aes(x=X1*0.5+0.2, y=X2*0.5+0.4)) +
  geom_smooth(method=lm, se=FALSE, lwd=2, aes(colour='beta_1')) + #lwd: thickness of the model line
  geom_segment(x=-100, xend=100, 
               y=intercept_pearson, yend=intercept_pearson, 
               lwd=2, aes(color="beta_0")) + 
  scale_color_manual(name=NULL, values=c("blue", "red"), labels=c(bquote(beta[0]*" (intercept)"), bquote(beta[1]*" (slope)")))
  
theme_axis(P_pearson, legend.position = c(0.4, 0.9))
```

### Unpacking the code chunk above
```{r}
# view the data frame created above
View(D_correlation)
```

```{r}
# try mine
guy = data.frame(MASS::mvrnorm(30, mu = c(0.9, 0.9), Sigma = matrix(c(1, 0.8, 1, 0.8), ncol = 2), empirical = TRUE))
View(guy)
```

```{r}
coefficients(fit)
```




















